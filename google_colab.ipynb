{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute only on Colab\n",
    "!pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute only on Colab\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image, image\n",
    "from torchsummary import summary\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2D2_ROOT = \"C:/Git/AUDI_A2D2_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ROOT = os.getcwd()\n",
    "\n",
    "trainingDatasetRoot = os.path.join(A2D2_ROOT, \"preprocessed\")\n",
    "classesPath = os.path.join(A2D2_ROOT, \"class_list.json\")\n",
    "\n",
    "predictionsDir = os.path.join(REPO_ROOT, \"predictions\")\n",
    "checkpointsDir = os.path.join(REPO_ROOT, \"checkpoints\")\n",
    "encodedTensorPath = os.path.join(REPO_ROOT, \"encoded_tensors\")\n",
    "\n",
    "if not os.path.exists(predictionsDir):\n",
    "    os.makedirs(predictionsDir)\n",
    "if not os.path.exists(checkpointsDir):\n",
    "    os.makedirs(checkpointsDir)\n",
    "if not os.path.exists(encodedTensorPath):\n",
    "    os.makedirs(encodedTensorPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "\n",
    "maxNumberOfFrames = None  # None -> full dataset\n",
    "encodingBatchSize = 64\n",
    "batchSize = 64  # Batch size of the TRAINING\n",
    "learning_rate = 0.01\n",
    "number_of_epochs = 30\n",
    "eval_per_batch = 5  # How many times in an epoch to evaluate the model\n",
    "chunk_size = 100\n",
    "log_frequency = 30  # TODO: function to make this automatically calculated based on the number of frames and eval_per_batch\n",
    "training_counter = 0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names: list = [\"Background\", \"Solid line\", \"Zebra crossing\", \"RD restricted area\", \"Drivable cobblestone\", \"Traffic guide obj.\", \"Dashed line\", \"RD normal street\"]\n",
    "training_results = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "@dataclass\n",
    "class CImageSize:\n",
    "    WIDTH: int = 416\n",
    "    HEIGHT: int = 224\n",
    "\n",
    "transformation_img = transforms.Compose([transforms.Resize((CImageSize.HEIGHT, CImageSize.WIDTH), antialias=True)])\n",
    "transformation_label = transforms.Compose([transforms.Resize((CImageSize.HEIGHT, CImageSize.WIDTH))])\n",
    "\n",
    "\n",
    "model = smp.DeepLabV3(\n",
    "    encoder_name=\"resnet34\", in_channels=3, encoder_depth=3, encoder_weights=\"imagenet\", aux_params=dict(dropout=0.35, classes=len(class_names))\n",
    ").to(device)\n",
    "encoder = model.encoder\n",
    "decoder = model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, (3, CImageSize.HEIGHT, CImageSize.WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(decoder.parameters(), learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "amp = ast.literal_eval(\"True\")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEncoderReader:\n",
    "    def __init__(self, f_datasetRoot: str, f_classesPath: str, f_maxNumberOfFrames: int = None) -> None:\n",
    "        self.m_datasetRoot: str = f_datasetRoot\n",
    "        self.m_framePaths: list[str] = []\n",
    "        self.m_labelPaths: list[str] = []\n",
    "        self.m_imageExtensions: list[str] = [\".jpg\", \".jpeg\", \".png\"]\n",
    "        self.m_labelClasses: dict[str, str] = {}\n",
    "        self.m_classesPath: str = f_classesPath\n",
    "\n",
    "        self.collectA2D2DataPaths()\n",
    "        self.loadClasses()\n",
    "\n",
    "        if f_maxNumberOfFrames:\n",
    "            self.m_framePaths = self.m_framePaths[:f_maxNumberOfFrames]\n",
    "            self.m_labelPaths = self.m_labelPaths[:f_maxNumberOfFrames]\n",
    "\n",
    "    def collectA2D2DataPaths(self) -> None:\n",
    "        \"\"\"Collect image paths and label paths from the A2D2 dataset.\"\"\"\n",
    "        l_sceneFolders: list[str] = [d for d in os.listdir(self.m_datasetRoot) if os.path.isdir(os.path.join(self.m_datasetRoot, d))]\n",
    "\n",
    "        for sceneFolder in l_sceneFolders:\n",
    "            frames_folders = [f.path for f in os.scandir(os.path.join(self.m_datasetRoot, sceneFolder, \"camera\")) if f.is_dir()]\n",
    "            labels_folders = [f.path for f in os.scandir(os.path.join(self.m_datasetRoot, sceneFolder, \"label\")) if f.is_dir()]\n",
    "\n",
    "            self.collectFramePaths(frames_folders)\n",
    "            self.collectLabelPaths(labels_folders)\n",
    "\n",
    "    def collectFramePaths(self, f_folders: list[str]) -> None:\n",
    "        for path in f_folders:\n",
    "            l_cameraImages = sorted([img for img in os.listdir(path) if os.path.splitext(img)[1].lower() in self.m_imageExtensions])\n",
    "            self.m_framePaths.extend([os.path.join(path, camera_img) for camera_img in l_cameraImages])\n",
    "\n",
    "    def collectLabelPaths(self, f_folders: list[str]) -> None:\n",
    "        for path in f_folders:\n",
    "            l_labelImages = sorted([img for img in os.listdir(path) if os.path.splitext(img)[1].lower() in self.m_imageExtensions])\n",
    "            self.m_labelPaths.extend([os.path.join(path, label_img) for label_img in l_labelImages])\n",
    "\n",
    "    def loadClasses(self) -> None:\n",
    "        with open(self.m_classesPath, \"r\") as json_file:\n",
    "            self.m_labelClasses = json.load(json_file)\n",
    "\n",
    "\n",
    "class CEncoderDataset(Dataset):\n",
    "    def __init__(self, f_reader: CEncoderReader, f_transformationImage: transforms.Compose, f_transformationLabel: transforms.Compose) -> None:\n",
    "        self.m_dataPaths: list[tuple[str, str]] = list(zip(f_reader.m_framePaths, f_reader.m_labelPaths))\n",
    "        self.m_transformationImage: transforms.Compose = f_transformationImage\n",
    "        self.m_transformationLabel: transforms.Compose = f_transformationLabel\n",
    "        self.m_filteredClasses: dict[str, int] = {k: v for k, v in f_reader.m_labelClasses.items() if v in class_names}\n",
    "        self.m_RGB2IDs: dict[tuple[int, int, int], int] = self.convertClassesToIDs(self.m_filteredClasses)\n",
    "        self.m_numberOfClasses: int = len(self.m_filteredClasses)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.m_dataPaths)\n",
    "\n",
    "    def __getitem__(self, f_index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        framePath, labelPath = self.m_dataPaths[f_index]\n",
    "\n",
    "        frame = read_image(framePath, mode=image.ImageReadMode.RGB)\n",
    "        # frame = self.m_transformationImage(frame)\n",
    "\n",
    "        label = read_image(labelPath, mode=image.ImageReadMode.RGB)\n",
    "        # label = self.m_transformationLabel(label)\n",
    "\n",
    "        id_map = CEncoderDataset.convertLabelToClassIDMap(label, self.m_RGB2IDs)\n",
    "\n",
    "        return torch.div(frame, 255).to(torch.float16).to(device), torch.nn.functional.one_hot(id_map.long(), num_classes=len(class_names)).permute(2, 0, 1).to(torch.int8)\n",
    "\n",
    "    @staticmethod\n",
    "    def convertHexaClassColorsToRBG(f_labelClasses: dict[int, int]) -> dict[str, tuple[int, int, int]]:\n",
    "        return {k: tuple(int(k.lstrip(\"#\")[i : i + 2], 16) for i in range(0, 6, 2)) for k in f_labelClasses.keys()}\n",
    "\n",
    "    @staticmethod\n",
    "    def convertClassesToIDs(f_labelClasses: dict[int, int]) -> dict[tuple[int, int, int], int]:\n",
    "        hexaToTGB = CEncoderDataset.convertHexaClassColorsToRBG(f_labelClasses)\n",
    "        return {r: i for i, r in enumerate(hexaToTGB.values())}\n",
    "\n",
    "    @staticmethod\n",
    "    def convertLabelToClassIDMap(f_label: torch.Tensor, f_RGB2IDs: dict[tuple[int, int, int], int]) -> torch.Tensor:\n",
    "        mask = torch.zeros(CImageSize.HEIGHT, CImageSize.WIDTH)\n",
    "        for rgb_code, class_id in f_RGB2IDs.items():\n",
    "            color_mask = f_label == torch.Tensor(rgb_code).reshape([3, 1, 1])\n",
    "            seg_mask = color_mask.sum(dim=0) == 3\n",
    "            mask[seg_mask] = class_id\n",
    "\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_to_encoder_reader = CEncoderReader(trainingDatasetRoot, classesPath, maxNumberOfFrames)\n",
    "training_to_encoder_dataset = CEncoderDataset(training_to_encoder_reader, transformation_img, transformation_label)\n",
    "training_to_encoder_loader = DataLoader(training_to_encoder_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_to_encoder_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_encoded_tensors():\n",
    "    with torch.no_grad():\n",
    "        for i, (frame, label) in enumerate(tqdm(training_to_encoder_loader)):\n",
    "            torch.save(encoder(frame)[-1].cpu().to(torch.float16), os.path.join(encodedTensorPath, f\"encoded_tensor_{i}.pt\"))\n",
    "            torch.save(label.cpu().to(torch.int8), os.path.join(encodedTensorPath, f\"encoded_label_{i}.pt\"))\n",
    "\n",
    "def encode_tensors():\n",
    "    encoder.half()\n",
    "    encoder.eval()\n",
    "\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    start_time = time()\n",
    "    save_encoded_tensors()\n",
    "    end_time = time()\n",
    "    print(f\"Encoding and saving time: {end_time - start_time:.2f} seconds for {maxNumberOfFrames} frames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTrainingResults(f_trainingResults: dict[str, list[float]], f_trainLoss: float, f_valLoss: float, f_trainAcc: float, f_valAcc: float) -> None:\n",
    "    f_trainingResults[\"train_loss\"].append(round(f_trainLoss, 4))\n",
    "    f_trainingResults[\"val_loss\"].append(round(f_valLoss, 4))\n",
    "    f_trainingResults[\"train_acc\"].append(round(f_trainAcc, 4))\n",
    "    f_trainingResults[\"val_acc\"].append(round(f_valAcc, 4))\n",
    "\n",
    "\n",
    "def savePredictions(f_masks: torch.Tensor, f_predicted: torch.Tensor, f_epoch: int, f_iter: int, f_trainingCnt: int) -> None:\n",
    "    num_images = min(f_masks.shape[0], 10)\n",
    "    _, axs = plt.subplots(num_images, 2, figsize=(10, 2 * num_images))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        axs[i, 0].imshow(f_masks[i].squeeze().cpu().numpy())\n",
    "        axs[i, 0].axis(\"off\")\n",
    "\n",
    "        axs[i, 1].imshow(f_predicted[i].squeeze().cpu().numpy())\n",
    "        axs[i, 1].axis(\"off\")\n",
    "\n",
    "    # Set title for each column\n",
    "    axs[0, 0].set_title(\"Label\")\n",
    "    axs[0, 1].set_title(\"Prediction\")\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "    plt.savefig(os.path.join(predictionsDir, f\"prediction_training_{f_trainingCnt}_epoch_{f_epoch}_iter_{f_iter}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def saveCheckpoint(f_name: str, f_decoder, f_optimizer, f_training_results, f_trainingCnt: int) -> None:\n",
    "    torch.save(\n",
    "        {\n",
    "            \"training_counter\": f_trainingCnt,\n",
    "            \"model_state_dict\": f_decoder.state_dict(),\n",
    "            \"optimizer_state_dict\": f_optimizer.state_dict(),\n",
    "            \"val_accuracy\": f_training_results[\"val_acc\"][-1],\n",
    "        },\n",
    "        os.path.join(checkpointsDir, f_name),  # Checkpoint dir is not param\n",
    "    )\n",
    "\n",
    "\n",
    "def logTrainingResults(f_epoch: int, f_trainingResults: dict[str, list[float]]) -> None:\n",
    "    print(\n",
    "        f\"Epoch {f_epoch} results:\\n \\\n",
    "        Training_loss: {f_trainingResults['train_loss'][-1]:.4f}, \\\n",
    "        Val_loss: {f_trainingResults['val_loss'][-1]:.4f}, \\\n",
    "        Train_accuracy: {f_trainingResults['train_acc'][-1]:.2f}%, \\\n",
    "        Val_accuracy: {f_trainingResults['val_acc'][-1]:.2f}%\"\n",
    "    )\n",
    "\n",
    "\n",
    "def train(f_trainingLoader: DataLoader, f_validationLoader: DataLoader, f_trainingResults: dict[str, list[float]], f_trainingCnt: int):\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Starting Training\")\n",
    "\n",
    "    for epoch in range(0, number_of_epochs):\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for i, batch_tr in enumerate(tqdm(f_trainingLoader)):\n",
    "            decoder.train()\n",
    "            inputs, masks = batch_tr[0].to(device), batch_tr[1].to(device).argmax(dim=1)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast(enabled=amp):\n",
    "\n",
    "                outputs = decoder(inputs)\n",
    "                outputs_upsampled = torch.nn.functional.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)\n",
    "                training_loss = criterion(outputs_upsampled, masks)\n",
    "                _, predicted = torch.max(outputs_upsampled, 1)\n",
    "                total += masks.nelement()\n",
    "                correct += (predicted == masks).sum().item()\n",
    "\n",
    "            scaler.scale(training_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            if i > 0 and (i % log_frequency == 0 or i == len(f_trainingLoader) - 1):\n",
    "                train_accuracy = 100 * correct / total\n",
    "                val_losses = []\n",
    "                correct, total = 0, 0\n",
    "                decoder.eval()\n",
    "                with torch.no_grad(), torch.cuda.amp.autocast(enabled=amp):\n",
    "\n",
    "                    for j, batch_vl in enumerate(f_validationLoader):\n",
    "                        inputs, masks = batch_vl[0].to(device), batch_vl[1].to(device).argmax(dim=1)\n",
    "                        outputs = decoder(inputs)\n",
    "                        outputs_upsampled = torch.nn.functional.interpolate(outputs, size=masks.shape[1:], mode=\"bilinear\", align_corners=False)\n",
    "                        val_loss = criterion(outputs_upsampled, masks)\n",
    "                        _, predicted = torch.max(outputs_upsampled, 1)\n",
    "                        total += masks.nelement()\n",
    "                        correct += (predicted == masks).sum().item()\n",
    "                        val_losses.append(val_loss)\n",
    "\n",
    "                        if j == 0:\n",
    "                            savePredictions(masks, predicted, epoch, i, f_trainingCnt)\n",
    "\n",
    "                avg_val_loss = torch.mean(torch.stack(val_losses))\n",
    "                val_accuracy = 100 * correct / total\n",
    "                updateTrainingResults(f_trainingResults, float(training_loss), float(avg_val_loss), train_accuracy, val_accuracy)\n",
    "\n",
    "        logTrainingResults(epoch, f_trainingResults)\n",
    "        # saveCheckpoint(epoch, f\"checkpoint_epoch_{epoch}.pth\", decoder, optimizer, f_trainingResults)\n",
    "\n",
    "    saveCheckpoint(f\"training_{f_trainingCnt}_checkpoint.pth\", decoder, optimizer, f_trainingResults, f_trainingCnt)\n",
    "    saveCheckpoint(f\"final_checkpoint.pth\", decoder, optimizer, f_trainingResults, f_trainingCnt)\n",
    "\n",
    "    df = pd.DataFrame(f_trainingResults)\n",
    "\n",
    "    df.to_csv(os.path.join(checkpointsDir, f\"training_{f_trainingCnt}_results.csv\"))\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.listdir(encodedTensorPath):\n",
    "    encode_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_to_encoder_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev_training = torch.load(os.path.join(REPO_ROOT, \"1_train_10_epochs_16_batch_full_dataset\", \"checkpoints\", \"final_checkpoint.pth\"))\n",
    "# decoder.load_state_dict(prev_training[\"model_state_dict\"])\n",
    "# optimizer.load_state_dict(prev_training[\"optimizer_state_dict\"])\n",
    "\n",
    "for i in range(0, len(training_to_encoder_loader), chunk_size):\n",
    "    print(f\"Training chunk {i // chunk_size + 1} of {len(training_to_encoder_loader) // chunk_size + 1}\")\n",
    "\n",
    "    if training_counter > 0:\n",
    "        checkpoint = torch.load(os.path.join(checkpointsDir, f\"training_{training_counter - 1}_checkpoint.pth\"))\n",
    "        decoder.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        print(\"Model and optimizer loaded\")\n",
    "\n",
    "    print(\"Loading encoded tensors and labels...\")\n",
    "    encoded_tensors_chunk = [\n",
    "        torch.load(os.path.join(encodedTensorPath, f\"encoded_tensor_{j}.pt\")).cpu() for j in tqdm(range(i, min(i + chunk_size, len(training_to_encoder_loader) - 1)))\n",
    "    ]\n",
    "    encoded_labels_chunk = [\n",
    "        torch.load(os.path.join(encodedTensorPath, f\"encoded_label_{j}.pt\")).cpu() for j in tqdm(range(i, min(i + chunk_size, len(training_to_encoder_loader) - 1)))\n",
    "    ]\n",
    "\n",
    "    encoded_tensors = torch.cat(encoded_tensors_chunk, dim=0)\n",
    "    encoded_labels = torch.cat(encoded_labels_chunk, dim=0)\n",
    "\n",
    "    dataset = TensorDataset(encoded_tensors, encoded_labels)\n",
    "\n",
    "    del encoded_tensors, encoded_labels\n",
    "\n",
    "    training_dataset, validation_dataset = torch.utils.data.random_split(dataset, [int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))])\n",
    "\n",
    "    training_loader = DataLoader(training_dataset, batch_size=batchSize, shuffle=True)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=batchSize, shuffle=False)\n",
    "\n",
    "    del dataset, training_dataset, validation_dataset\n",
    "\n",
    "    # log_frequency = # max(int(int(len(training_loader) / batchSize) / eval_per_batch), 1)\n",
    "\n",
    "    train(training_loader, validation_loader, training_results, training_counter)\n",
    "    training_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join(checkpointsDir, \"final_checkpoint.pth\"))\n",
    "model = smp.DeepLabV3(encoder_name=\"resnet34\", in_channels=3, encoder_depth=4, classes=len(class_names), encoder_weights=\"imagenet\").to(device)\n",
    "model.decoder.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"20181204154421_camera_frontcenter_000043949.png\"\n",
    "image_path = os.path.join(A2D2_ROOT, \"training\", \"20181204_154421\", \"camera\", \"cam_front_center\", image_name)\n",
    "label_name = image_name.replace(\"camera\", \"label\")\n",
    "label_path = os.path.join(A2D2_ROOT, \"training\", \"20181204_154421\", \"label\", \"label_frontcenter\", label_name)\n",
    "input_data = read_image(image_path, mode=image.ImageReadMode.RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prediction = model(torch.div(transformation(input_data.unsqueeze(0)), 255).to(device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = torch.argmax(prediction, dim=1).squeeze().cpu().numpy()\n",
    "input_data_transposed = input_data.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# Visualize the original image and the prediction\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(input_data_transposed)\n",
    "plt.title(\"Original Image\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(class_map)\n",
    "plt.title(\"Prediction\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
